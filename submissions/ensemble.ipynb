{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jarroyo/miniforge3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/jarroyo/miniforge3/lib/python3.9/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/LOANS_TRAIN.csv')\n",
    "test_data = pd.read_csv('../data/LOANS_TEST.csv')\n",
    "\n",
    "id_column = test_data['id']\n",
    "\n",
    "train_data.drop(columns=['id','grade', 'emp_title', 'title'], axis=1, inplace=True)\n",
    "test_data.drop(columns=['id','grade', 'emp_title', 'title'], axis=1, inplace=True)\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder2 = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "train_data['sub_grade'] = labelencoder.fit_transform(train_data['sub_grade'])\n",
    "train_data['home_ownership'] = labelencoder.fit_transform(train_data['home_ownership'])\n",
    "train_data['emp_length'].replace('< 1 year', 0.5, inplace=True)\n",
    "train_data['emp_length'].replace('1 year', 1.0, inplace=True)\n",
    "train_data['emp_length'].replace('2 years', 2.0, inplace=True)\n",
    "train_data['emp_length'].replace('3 years', 3.0, inplace=True)\n",
    "train_data['emp_length'].replace('4 years', 4.0, inplace=True)\n",
    "train_data['emp_length'].replace('5 years', 5.0, inplace=True)\n",
    "train_data['emp_length'].replace('6 years', 6.0, inplace=True)\n",
    "train_data['emp_length'].replace('7 years', 7.0, inplace=True)\n",
    "train_data['emp_length'].replace('8 years', 8.0, inplace=True)\n",
    "train_data['emp_length'].replace('9 years', 9.0, inplace=True)\n",
    "train_data['emp_length'].replace('10 years', 10.0, inplace=True)\n",
    "train_data['emp_length'].replace('10+ years', 15.0, inplace=True)\n",
    "train_data['emp_length'] = train_data['emp_length'].fillna(0)\n",
    "\n",
    "test_data['sub_grade'] = labelencoder2.fit_transform(test_data['sub_grade'])\n",
    "test_data['home_ownership'] = labelencoder2.fit_transform(test_data['home_ownership'])\n",
    "test_data['emp_length'].replace('< 1 year', 0.5, inplace=True)\n",
    "test_data['emp_length'].replace('1 year', 1.0, inplace=True)\n",
    "test_data['emp_length'].replace('2 years', 2.0, inplace=True)\n",
    "test_data['emp_length'].replace('3 years', 3.0, inplace=True)\n",
    "test_data['emp_length'].replace('4 years', 4.0, inplace=True)\n",
    "test_data['emp_length'].replace('5 years', 5.0, inplace=True)\n",
    "test_data['emp_length'].replace('6 years', 6.0, inplace=True)\n",
    "test_data['emp_length'].replace('7 years', 7.0, inplace=True)\n",
    "test_data['emp_length'].replace('8 years', 8.0, inplace=True)\n",
    "test_data['emp_length'].replace('9 years', 9.0, inplace=True)\n",
    "test_data['emp_length'].replace('10 years', 10.0, inplace=True)\n",
    "test_data['emp_length'].replace('10+ years', 15.0, inplace=True)\n",
    "test_data['emp_length'] = test_data['emp_length'].fillna(0)\n",
    "\n",
    "train_data['mort_acc'] = train_data['mort_acc'].fillna(0)\n",
    "\n",
    "test_data['mort_acc'] = test_data['mort_acc'].fillna(0)\n",
    "# Strip percent(%) from int_rate\n",
    "train_data['int_rate'] = train_data['int_rate'].str.rstrip('%').astype(float)\n",
    "test_data['int_rate'] = test_data['int_rate'].str.rstrip('%').astype(float)\n",
    "\n",
    "#Strip percent(%) from revol_util\n",
    "train_data['revol_util'] = train_data['revol_util'].str.rstrip('%').astype(float)\n",
    "test_data['revol_util'] = test_data['revol_util'].str.rstrip('%').astype(float)\n",
    "\n",
    "X_train = train_data.iloc[:,:-1]\n",
    "y_train = train_data.iloc[:,-1]\n",
    "X_test = test_data.iloc[:,:]\n",
    "\n",
    "X_train_numeric = X_train.select_dtypes(include=np.number)\n",
    "X_test_numeric = X_test.select_dtypes(include=np.number)\n",
    "# y_train_numeric = y_train.select_dtypes(include=np.number)\n",
    "y_train_numeric = y_train.copy(deep=False)\n",
    "y_train_numeric.replace('Fully Paid', 0.0, inplace=True)\n",
    "y_train_numeric.replace('Charged Off', 1.0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([180706,   8783, 126927,  81143, 194946, 166860,   8811, 151014,\n",
      "            186961,  64670,\n",
      "            ...\n",
      "            134835, 156149, 113995, 189779,   3822, 129087,  38876,   7901,\n",
      "             32524, 115271],\n",
      "           dtype='int64', length=157800)\n"
     ]
    }
   ],
   "source": [
    "X_val_numeric = X_train_numeric.sample(frac=0.2)\n",
    "val_indices = X_val_numeric.index\n",
    "X_train_numeric = X_train_numeric.drop(val_indices)\n",
    "train_indices = X_train_numeric.index\n",
    "Y_val = y_train.drop(train_indices)\n",
    "y_train = y_train.drop(val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=10, min_child_weight=2,  n_estimators=100,\n",
    "                          n_jobs=-1, verbose=1,learning_rate=0.01, gamma=4, random_state=0)\n",
    "model.fit(X_train_numeric, y_train_numeric)\n",
    "xgb_predictions = model.predict_proba(X_val_numeric)[:,1]\n",
    "print('AUC XGB')\n",
    "print(sklearn.metrics.roc_auc_score(Y_val, xgb_predictions[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF XGBOOST TRAINING, START OF CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/LOANS_TRAIN.csv')\n",
    "test_data = pd.read_csv('./data/LOANS_TEST.csv')\n",
    "\n",
    "id_column = test_data['id']\n",
    "\n",
    "train_data.drop(columns=['id','grade', 'emp_title', 'title', 'earliest_cr_line', 'issue_d', 'zip_code'], axis=1, inplace=True)\n",
    "test_data.drop(columns=['id','grade', 'emp_title', 'title', 'earliest_cr_line', 'issue_d', 'zip_code'], axis=1, inplace=True)\n",
    "\n",
    "# we want to ultimately use this data, but its nominal multi-categorical nature requires further preprocessing\n",
    "# prolly with OneHotEncoder\n",
    "    # Deal with nominal, multi-categorical data\n",
    "    # Goal: convert home_ownership (RENT, OWN, MORTGAGE, OTHER) to a 4D vector.\n",
    "        # if MORTGAGE ==> [0,0,1,0]\n",
    "# print(len(train_data['zip_code'].unique()))\n",
    "# print(train_data['zip_code'].unique())\n",
    "ce_OHE = ce.OneHotEncoder(cols=['addr_state', 'home_ownership', 'purpose'])\n",
    "print('BEFORE')\n",
    "print(train_data.shape)\n",
    "train_data = ce_OHE.fit_transform(train_data)\n",
    "print('AFTER')\n",
    "print(train_data.shape)\n",
    "test_data = ce_OHE.fit_transform(test_data)\n",
    "# NOTE: 'purpose' only has 14 categories, probably good things to learn from\n",
    "\n",
    "# there are no joint applications in training, so model won't be able to learn it. Better to drop it\n",
    "train_data.drop(columns=['application_type', 'purpose_14'], axis=1, inplace=True)\n",
    "test_data.drop(columns=['application_type'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder2 = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "train_data['sub_grade'] = labelencoder.fit_transform(train_data['sub_grade'])\n",
    "train_data['emp_length'].replace('< 1 year', 0, inplace=True)\n",
    "train_data['emp_length'].replace('1 year', 1.0, inplace=True)\n",
    "train_data['emp_length'].replace('2 years', 2.0, inplace=True)\n",
    "train_data['emp_length'].replace('3 years', 3.0, inplace=True)\n",
    "train_data['emp_length'].replace('4 years', 4.0, inplace=True)\n",
    "train_data['emp_length'].replace('5 years', 5.0, inplace=True)\n",
    "train_data['emp_length'].replace('6 years', 6.0, inplace=True)\n",
    "train_data['emp_length'].replace('7 years', 7.0, inplace=True)\n",
    "train_data['emp_length'].replace('8 years', 8.0, inplace=True)\n",
    "train_data['emp_length'].replace('9 years', 9.0, inplace=True)\n",
    "train_data['emp_length'].replace('10 years', 10.0, inplace=True)\n",
    "train_data['emp_length'].replace('10+ years', 15.0, inplace=True)\n",
    "train_data['emp_length'] = train_data['emp_length'].fillna(0)\n",
    "train_data['pub_rec_bankruptcies'] = train_data['pub_rec_bankruptcies'].fillna(0)\n",
    "train_data['verification_status'].replace('Verified', 1, inplace=True)\n",
    "train_data['verification_status'].replace('Source Verified', 1, inplace=True)\n",
    "train_data['verification_status'].replace('Not Verified', 0, inplace=True)\n",
    "train_data['initial_list_status'].replace('w', 1, inplace=True)\n",
    "train_data['initial_list_status'].replace('f', 0, inplace=True)\n",
    "\n",
    "test_data['sub_grade'] = labelencoder2.fit_transform(test_data['sub_grade'])\n",
    "test_data['emp_length'].replace('< 1 year', 0, inplace=True)\n",
    "test_data['emp_length'].replace('1 year', 1.0, inplace=True)\n",
    "test_data['emp_length'].replace('2 years', 2.0, inplace=True)\n",
    "test_data['emp_length'].replace('3 years', 3.0, inplace=True)\n",
    "test_data['emp_length'].replace('4 years', 4.0, inplace=True)\n",
    "test_data['emp_length'].replace('5 years', 5.0, inplace=True)\n",
    "test_data['emp_length'].replace('6 years', 6.0, inplace=True)\n",
    "test_data['emp_length'].replace('7 years', 7.0, inplace=True)\n",
    "test_data['emp_length'].replace('8 years', 8.0, inplace=True)\n",
    "test_data['emp_length'].replace('9 years', 9.0, inplace=True)\n",
    "test_data['emp_length'].replace('10 years', 10.0, inplace=True)\n",
    "test_data['emp_length'].replace('10+ years', 15.0, inplace=True)\n",
    "test_data['emp_length'] = test_data['emp_length'].fillna(0)\n",
    "test_data['pub_rec_bankruptcies'] = test_data['pub_rec_bankruptcies'].fillna(0)\n",
    "test_data['verification_status'].replace('Verified', 1, inplace=True)\n",
    "# technically encoding two categories into one... might want to change\n",
    "test_data['verification_status'].replace('Verified', 1, inplace=True)\n",
    "test_data['verification_status'].replace('Source Verified', 1, inplace=True)\n",
    "test_data['verification_status'].replace('Not Verified', 0, inplace=True)\n",
    "test_data['initial_list_status'].replace('w', 1, inplace=True)\n",
    "test_data['initial_list_status'].replace('f', 0, inplace=True)\n",
    "\n",
    "train_data['mort_acc'] = train_data['mort_acc'].fillna(0)\n",
    "\n",
    "test_data['mort_acc'] = test_data['mort_acc'].fillna(0)\n",
    "# Strip percent(%) from int_rate\n",
    "train_data['int_rate'] = train_data['int_rate'].str.rstrip('%').astype(float)\n",
    "test_data['int_rate'] = test_data['int_rate'].str.rstrip('%').astype(float)\n",
    "\n",
    "#Strip percent(%) from revol_util\n",
    "train_data['revol_util'] = train_data['revol_util'].str.rstrip('%').astype(float)\n",
    "test_data['revol_util'] = test_data['revol_util'].str.rstrip('%').astype(float)\n",
    "train_data['revol_util'] = train_data['revol_util'].fillna(0)\n",
    "test_data['revol_util'] = test_data['revol_util'].fillna(0)\n",
    "\n",
    "X_train = train_data.iloc[:,:-1]\n",
    "y_train = train_data.iloc[:,-1]\n",
    "X_test = test_data.iloc[:,:]\n",
    "X_train_numeric = X_train.select_dtypes(include=np.number)\n",
    "X_test_numeric = X_test.select_dtypes(include=np.number)\n",
    "# y_train_numeric = y_train.select_dtypes(include=np.number)\n",
    "y_train_numeric = y_train.copy(deep=False)\n",
    "y_train_numeric.replace('Fully Paid', 0.0, inplace=True)\n",
    "y_train_numeric.replace('Charged Off', 1.0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_numeric = X_train_numeric.drop(train_indices)\n",
    "Y_val = y_train.drop(train_indices)\n",
    "X_train_numeric = X_train_numeric.drop(val_indices)\n",
    "y_train = y_train.drop(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train_numeric.columns:\n",
    "    if X_train_numeric[col].isnull().values.any() > 0:\n",
    "        print(col)\n",
    "    x_min = X_train_numeric[col].min()\n",
    "    x_max = X_train_numeric[col].max()\n",
    "    X_train_numeric[col] = (X_train_numeric[col] - x_min) / (x_max - x_min)\n",
    "    # also need to normalize test data, but must use training data\n",
    "    X_test_numeric[col] = (X_test_numeric[col] - x_min) / (x_max - x_min)\n",
    "    X_test_numeric[col].mask(X_test_numeric[col] < 0, 0, inplace=True)\n",
    "    X_test_numeric[col].mask(X_test_numeric[col] > 1, 1, inplace=True)\n",
    "    # TODO: DETERMINE WHAT TO DO IN THESE CASES!!!\n",
    "    if X_test_numeric[col].min() < 0:\n",
    "        print(f\"min is less than zero in test data in column {col}\")\n",
    "        print(X_test_numeric[col].min())\n",
    "    if X_test_numeric[col].max() > 1:\n",
    "        print(f\"max is more than one in test data in column {col}\")\n",
    "        print(X_test_numeric[col].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, 3), stride=1, padding=0),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=(1, 2), stride=1),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1, 3), stride=1, padding=0),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=(1, 2), stride=1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(5056, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model has some # of parameters:\n",
    "count = 0\n",
    "for p in model.parameters():\n",
    "    n_params = np.prod(list(p.data.shape)).item()\n",
    "    count += n_params\n",
    "print(f'total params: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        \"\"\"\n",
    "        X: a dataframe with rows as training examples and columns features\n",
    "        Y: a pandas series with labels\"\"\"\n",
    "        self.x_train=torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(Y.values, dtype=torch.float32)\n",
    "        self.x_train = torch.reshape(self.x_train, (self.x_train.shape[0], 1, 1, self.x_train.shape[1],))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.x_train[idx], self.y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "ds_train = CustomDataset(X_train_numeric, y_train_numeric)\n",
    "ds_val = CustomDataset(X_val_numeric, Y_val)\n",
    "train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_eval_loader = DataLoader(ds_val, batch_size=len(ds_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10  # paper used 100 originally\n",
    "lr = 0.001\n",
    "\n",
    "train_loss_hist = np.zeros([n_epochs, 1])\n",
    "val_loss_hist = np.zeros([n_epochs, 1])\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'EPOCH {epoch}')\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        features, labels = data\n",
    "        # print(type(features))\n",
    "        # print(f\"PESKY BASTARDS: {features.isnan().sum()}\")\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(features)\n",
    "        output = torch.reshape(output, (output.shape[0], ))\n",
    "        if output.isnan().sum() > 0:\n",
    "            print(f\"RIP{output.isnan().sum()}\")\n",
    "        output = torch.nan_to_num(output)\n",
    "        # calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update\n",
    "        optimizer.step()\n",
    "        # track training loss\n",
    "        train_loss_hist[epoch] += loss.item()\n",
    "    train_loss_hist[epoch] /= len(train_loader)\n",
    "    print(f\"\\ttraining loss: {train_loss_hist[epoch]}\")\n",
    "\n",
    "    # validate\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = None\n",
    "        labels = None\n",
    "        for i, data in enumerate(val_eval_loader):\n",
    "            features, labels = data\n",
    "            output = model(features)\n",
    "            output = torch.reshape(output, (output.shape[0], ))\n",
    "            output = torch.nan_to_num(output)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss_hist[epoch] += loss.item()\n",
    "        preds = output.cpu().detach().numpy()\n",
    "        ground_truths = labels.cpu().detach().numpy()\n",
    "        print(f'\\tAUC: {sklearn.metrics.roc_auc_score(ground_truths, preds)}')\n",
    "        val_loss_hist[epoch] /= len(val_eval_loader)\n",
    "        print(f\"\\tvalidation loss: {val_loss_hist[epoch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "cnn_predictions = None\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = None\n",
    "    labels = None\n",
    "    for i, data in enumerate(val_eval_loader):\n",
    "        features, labels = data\n",
    "        output = model(features)\n",
    "        output = torch.reshape(output, (output.shape[0], ))\n",
    "        output = torch.nan_to_num(output)\n",
    "        loss = criterion(output, labels)\n",
    "        val_loss_hist[epoch] += loss.item()\n",
    "    preds = output.cpu().detach().numpy()\n",
    "    cnn_predictions = preds\n",
    "    ground_truths = labels.cpu().detach().numpy()\n",
    "    print(f'\\tAUC: {sklearn.metrics.roc_auc_score(ground_truths, preds)}')\n",
    "    val_loss_hist[epoch] /= len(val_eval_loader)\n",
    "    print(f\"\\tvalidation loss: {val_loss_hist[epoch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create points of the form (1, xgb pred, cnn pred) and plot them by color according to their ground truth label\n",
    "# see if they are linearly separable\n",
    "assert len(cnn_predictions) == len(xgb_predictions)\n",
    "assert len(ground_truths) == len(cnn_predictions)\n",
    "\n",
    "paid = {\"xgb\": [], \"cnn\": []}\n",
    "not_paid = {\"xgb\": [], \"cnn\": []}\n",
    "for i in range(cnn_predictions):\n",
    "    if ground_truths[i] == 0:\n",
    "        paid[\"xgb\"].append(xgb_predictions[i])\n",
    "        paid[\"cnn\"].append(cnn_predictions[i])\n",
    "    else:\n",
    "        not_paid[\"xgb\"].append(xgb_predictions[i])\n",
    "        not_paid[\"cnn\"].append(cnn_predictions[i])\n",
    "\n",
    "plt.scatter(paid[\"xgb\"], paid[\"cnn\"])\n",
    "plt.scatter(not_paid[\"xgb\"], not_paid[\"cnn\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Dataset\n",
    "\n",
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, X):\n",
    "#         \"\"\"\n",
    "#         X: a dataframe with rows as training examples and columns features\n",
    "#         Y: a pandas series with labels\"\"\"\n",
    "#         self.x_train=torch.tensor(X.values, dtype=torch.float32)\n",
    "#         self.x_train = torch.reshape(self.x_train, (self.x_train.shape[0], 1, 1, self.x_train.shape[1],))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         print(f'GETTING LENGTH {self.x_train.shape[0]}')\n",
    "#         return self.x_train.shape[0]\n",
    "\n",
    "#     def __getitem__(self,idx):\n",
    "#         return self.x_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test = TestDataset(X_test_numeric)\n",
    "\n",
    "# test_loader = torch.utils.data.Dataloader(ds_test, batch_size=1, shuffle=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     cnn_predictions = []\n",
    "#     for i, data in enumerate(test_loader):\n",
    "#         features = data\n",
    "#         output = model(features)\n",
    "#         output = torch.reshape(output, (output.shape[0], ))\n",
    "#         if output.isnan().sum() > 0:\n",
    "#             print(f\"RIP{output.isnan().sum()}\")\n",
    "#         output = torch.nan_to_num(output)\n",
    "#         print(f'PREDICTION: {output.item()}')\n",
    "#         cnn_predictions.append(output.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_features = []\n",
    "\n",
    "assert len(cnn_predictions) == len(xgb_predictions)\n",
    "\n",
    "for i in range(len(cnn_predictions)):\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b5295d72cc8c3d140bbb6686d5919ce0ad0a523816efde1e1cd082b7d39dbc7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
