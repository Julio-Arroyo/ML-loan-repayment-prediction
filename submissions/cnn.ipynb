{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jarroyo/miniforge3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/jarroyo/miniforge3/lib/python3.9/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to numeric:\n",
    "- ordinal variables encoded as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n",
      "(197250, 22)\n",
      "AFTER\n",
      "(197250, 88)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/LOANS_TRAIN.csv')\n",
    "test_data = pd.read_csv('../data/LOANS_TEST.csv')\n",
    "\n",
    "id_column = test_data['id']\n",
    "\n",
    "train_data.drop(columns=['id','grade', 'emp_title', 'title', 'earliest_cr_line', 'issue_d', 'zip_code'], axis=1, inplace=True)\n",
    "test_data.drop(columns=['id','grade', 'emp_title', 'title', 'earliest_cr_line', 'issue_d', 'zip_code'], axis=1, inplace=True)\n",
    "\n",
    "# we want to ultimately use this data, but its nominal multi-categorical nature requires further preprocessing\n",
    "# prolly with OneHotEncoder\n",
    "    # Deal with nominal, multi-categorical data\n",
    "    # Goal: convert home_ownership (RENT, OWN, MORTGAGE, OTHER) to a 4D vector.\n",
    "        # if MORTGAGE ==> [0,0,1,0]\n",
    "# print(len(train_data['zip_code'].unique()))\n",
    "# print(train_data['zip_code'].unique())\n",
    "ce_OHE = ce.OneHotEncoder(cols=['addr_state', 'home_ownership', 'purpose'])\n",
    "print('BEFORE')\n",
    "print(train_data.shape)\n",
    "train_data = ce_OHE.fit_transform(train_data)\n",
    "print('AFTER')\n",
    "print(train_data.shape)\n",
    "test_data = ce_OHE.fit_transform(test_data)\n",
    "# NOTE: 'purpose' only has 14 categories, probably good things to learn from\n",
    "\n",
    "# there are no joint applications in training, so model won't be able to learn it. Better to drop it\n",
    "train_data.drop(columns=['application_type', 'purpose_14'], axis=1, inplace=True)\n",
    "test_data.drop(columns=['application_type'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder2 = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "train_data['sub_grade'] = labelencoder.fit_transform(train_data['sub_grade'])\n",
    "train_data['emp_length'].replace('< 1 year', 0, inplace=True)\n",
    "train_data['emp_length'].replace('1 year', 1.0, inplace=True)\n",
    "train_data['emp_length'].replace('2 years', 2.0, inplace=True)\n",
    "train_data['emp_length'].replace('3 years', 3.0, inplace=True)\n",
    "train_data['emp_length'].replace('4 years', 4.0, inplace=True)\n",
    "train_data['emp_length'].replace('5 years', 5.0, inplace=True)\n",
    "train_data['emp_length'].replace('6 years', 6.0, inplace=True)\n",
    "train_data['emp_length'].replace('7 years', 7.0, inplace=True)\n",
    "train_data['emp_length'].replace('8 years', 8.0, inplace=True)\n",
    "train_data['emp_length'].replace('9 years', 9.0, inplace=True)\n",
    "train_data['emp_length'].replace('10 years', 10.0, inplace=True)\n",
    "train_data['emp_length'].replace('10+ years', 15.0, inplace=True)\n",
    "train_data['emp_length'] = train_data['emp_length'].fillna(0)\n",
    "train_data['pub_rec_bankruptcies'] = train_data['pub_rec_bankruptcies'].fillna(0)\n",
    "train_data['verification_status'].replace('Verified', 1, inplace=True)\n",
    "train_data['verification_status'].replace('Source Verified', 1, inplace=True)\n",
    "train_data['verification_status'].replace('Not Verified', 0, inplace=True)\n",
    "train_data['initial_list_status'].replace('w', 1, inplace=True)\n",
    "train_data['initial_list_status'].replace('f', 0, inplace=True)\n",
    "\n",
    "test_data['sub_grade'] = labelencoder2.fit_transform(test_data['sub_grade'])\n",
    "test_data['emp_length'].replace('< 1 year', 0, inplace=True)\n",
    "test_data['emp_length'].replace('1 year', 1.0, inplace=True)\n",
    "test_data['emp_length'].replace('2 years', 2.0, inplace=True)\n",
    "test_data['emp_length'].replace('3 years', 3.0, inplace=True)\n",
    "test_data['emp_length'].replace('4 years', 4.0, inplace=True)\n",
    "test_data['emp_length'].replace('5 years', 5.0, inplace=True)\n",
    "test_data['emp_length'].replace('6 years', 6.0, inplace=True)\n",
    "test_data['emp_length'].replace('7 years', 7.0, inplace=True)\n",
    "test_data['emp_length'].replace('8 years', 8.0, inplace=True)\n",
    "test_data['emp_length'].replace('9 years', 9.0, inplace=True)\n",
    "test_data['emp_length'].replace('10 years', 10.0, inplace=True)\n",
    "test_data['emp_length'].replace('10+ years', 15.0, inplace=True)\n",
    "test_data['emp_length'] = test_data['emp_length'].fillna(0)\n",
    "test_data['pub_rec_bankruptcies'] = test_data['pub_rec_bankruptcies'].fillna(0)\n",
    "test_data['verification_status'].replace('Verified', 1, inplace=True)\n",
    "# technically encoding two categories into one... might want to change\n",
    "test_data['verification_status'].replace('Verified', 1, inplace=True)\n",
    "test_data['verification_status'].replace('Source Verified', 1, inplace=True)\n",
    "test_data['verification_status'].replace('Not Verified', 0, inplace=True)\n",
    "test_data['initial_list_status'].replace('w', 1, inplace=True)\n",
    "test_data['initial_list_status'].replace('f', 0, inplace=True)\n",
    "\n",
    "train_data['mort_acc'] = train_data['mort_acc'].fillna(0)\n",
    "\n",
    "test_data['mort_acc'] = test_data['mort_acc'].fillna(0)\n",
    "# Strip percent(%) from int_rate\n",
    "train_data['int_rate'] = train_data['int_rate'].str.rstrip('%').astype(float)\n",
    "test_data['int_rate'] = test_data['int_rate'].str.rstrip('%').astype(float)\n",
    "\n",
    "#Strip percent(%) from revol_util\n",
    "train_data['revol_util'] = train_data['revol_util'].str.rstrip('%').astype(float)\n",
    "test_data['revol_util'] = test_data['revol_util'].str.rstrip('%').astype(float)\n",
    "train_data['revol_util'] = train_data['revol_util'].fillna(0)\n",
    "test_data['revol_util'] = test_data['revol_util'].fillna(0)\n",
    "\n",
    "X_train = train_data.iloc[:,:-1]\n",
    "y_train = train_data.iloc[:,-1]\n",
    "X_test = test_data.iloc[:,:]\n",
    "X_train_numeric = X_train.select_dtypes(include=np.number)\n",
    "X_test_numeric = X_test.select_dtypes(include=np.number)\n",
    "# y_train_numeric = y_train.select_dtypes(include=np.number)\n",
    "y_train_numeric = y_train.copy(deep=False)\n",
    "y_train_numeric.replace('Fully Paid', 0.0, inplace=True)\n",
    "y_train_numeric.replace('Charged Off', 1.0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs take vectors with values in [0,1]. Here we normalize the numberical data accordingly with min-max normalization.\n",
    "Standardize an arbitrary feature X by using\n",
    "\n",
    "X' = (X - X_min)/(X_max - X_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train_numeric.columns:\n",
    "    if X_train_numeric[col].isnull().values.any() > 0:\n",
    "        print(col)\n",
    "    x_min = X_train_numeric[col].min()\n",
    "    x_max = X_train_numeric[col].max()\n",
    "    X_train_numeric[col] = (X_train_numeric[col] - x_min) / (x_max - x_min)\n",
    "    # also need to normalize test data, but must use training data\n",
    "    X_test_numeric[col] = (X_test_numeric[col] - x_min) / (x_max - x_min)\n",
    "    X_test_numeric[col].mask(X_test_numeric[col] < 0, 0, inplace=True)\n",
    "    X_test_numeric[col].mask(X_test_numeric[col] > 1, 1, inplace=True)\n",
    "    # TODO: DETERMINE WHAT TO DO IN THESE CASES!!!\n",
    "    if X_test_numeric[col].min() < 0:\n",
    "        print(f\"min is less than zero in test data in column {col}\")\n",
    "        print(X_test_numeric[col].min())\n",
    "    if X_test_numeric[col].max() > 1:\n",
    "        print(f\"max is more than one in test data in column {col}\")\n",
    "        print(X_test_numeric[col].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, 3), stride=1, padding=0),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.MaxPool2d(kernel_size=(1, 2), stride=1),\n",
    "#     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1, 3), stride=1, padding=0),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.MaxPool2d(kernel_size=(1, 2), stride=1),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(5056, 512),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.25),\n",
    "#     nn.Linear(512, 1),\n",
    "#     nn.Sigmoid()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model has some # of parameters:\n",
    "# count = 0\n",
    "# for p in model.parameters():\n",
    "#     n_params = np.prod(list(p.data.shape)).item()\n",
    "#     count += n_params\n",
    "# print(f'total params: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13043478 0.         0.25339147 ... 0.         0.         0.        ]\n",
      " [0.05797101 1.         0.47722868 ... 0.         0.         0.        ]\n",
      " [0.05507246 0.         0.51065891 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.30434783 0.         0.42005814 ... 0.         0.         0.        ]\n",
      " [0.89565217 1.         0.84932171 ... 0.         0.12903226 0.        ]\n",
      " [0.14492754 0.         0.42005814 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_numeric.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a Dataset object, which we make from the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        \"\"\"\n",
    "        X: a dataframe with rows as training examples and columns features\n",
    "        Y: a pandas series with labels\"\"\"\n",
    "        self.x_train=torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(Y.values, dtype=torch.float32)\n",
    "        self.x_train = torch.reshape(self.x_train, (self.x_train.shape[0], 1, 1, self.x_train.shape[1],))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.x_train[idx], self.y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# BATCH_SIZE = 512\n",
    "\n",
    "# ds_train = CustomDataset(X_train_numeric, y_train_numeric)\n",
    "# len_full = len(ds_train)\n",
    "# len_train = int(0.8*len(ds_train))\n",
    "# len_val = len_full - len_train\n",
    "# ds_train, ds_val = random_split(ds_train, [len_train, len_val])\n",
    "# train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_eval_loader = DataLoader(ds_val, batch_size=len_val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 50  # paper used 100 originally\n",
    "# lr = 0.001\n",
    "\n",
    "# train_loss_hist = np.zeros([n_epochs, 1])\n",
    "# val_loss_hist = np.zeros([n_epochs, 1])\n",
    "# for epoch in range(n_epochs):\n",
    "#     print(f'EPOCH {epoch}')\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for i, data in enumerate(train_loader):\n",
    "#         features, labels = data\n",
    "#         # print(type(features))\n",
    "#         # print(f\"PESKY BASTARDS: {features.isnan().sum()}\")\n",
    "#         optimizer.zero_grad()\n",
    "#         # forward pass\n",
    "#         output = model(features)\n",
    "#         output = torch.reshape(output, (output.shape[0], ))\n",
    "#         if output.isnan().sum() > 0:\n",
    "#             print(f\"RIP{output.isnan().sum()}\")\n",
    "#         output = torch.nan_to_num(output)\n",
    "#         # calculate loss\n",
    "#         loss = criterion(output, labels)\n",
    "#         # backward pass\n",
    "#         loss.backward()\n",
    "#         # update\n",
    "#         optimizer.step()\n",
    "#         # track training loss\n",
    "#         train_loss_hist[epoch] += loss.item()\n",
    "#     train_loss_hist[epoch] /= len(train_loader)\n",
    "#     print(f\"\\ttraining loss: {train_loss_hist[epoch]}\")\n",
    "\n",
    "#     # validate\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         output = None\n",
    "#         labels = None\n",
    "#         for i, data in enumerate(val_eval_loader):\n",
    "#             features, labels = data\n",
    "#             output = model(features)\n",
    "#             output = torch.reshape(output, (output.shape[0], ))\n",
    "#             output = torch.nan_to_num(output)\n",
    "#             loss = criterion(output, labels)\n",
    "#             val_loss_hist[epoch] += loss.item()\n",
    "#         preds = output.cpu().detach().numpy()\n",
    "#         ground_truths = labels.cpu().detach().numpy()\n",
    "#         print(f'\\tAUC: {sklearn.metrics.roc_auc_score(ground_truths, preds)}')\n",
    "#         val_loss_hist[epoch] /= len(val_eval_loader)\n",
    "#         print(f\"\\tvalidation loss: {val_loss_hist[epoch]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPOUT RATE: 0.0\n",
      "FOLD 0\n",
      "EPOCH 0\n",
      "\ttraining loss: 0.42194286403532555\n",
      "\tAUC: 0.6816580293146692\n",
      "\tvalidation loss: 0.40277963876724243\n",
      "EPOCH 1\n",
      "\ttraining loss: 0.4034852230240226\n",
      "\tAUC: 0.6837978981208627\n",
      "\tvalidation loss: 0.40282630920410156\n",
      "EPOCH 2\n",
      "\ttraining loss: 0.40193394578776315\n",
      "\tAUC: 0.6846275792003989\n",
      "\tvalidation loss: 0.4042237401008606\n",
      "EPOCH 3\n",
      "\ttraining loss: 0.40236387476566154\n",
      "\tAUC: 0.683870128000696\n",
      "\tvalidation loss: 0.40134403109550476\n",
      "EPOCH 4\n",
      "\ttraining loss: 0.4009619558706253\n",
      "\tAUC: 0.6829295377999969\n",
      "\tvalidation loss: 0.4019487798213959\n",
      "EPOCH 5\n",
      "\ttraining loss: 0.40170835744601624\n",
      "\tAUC: 0.6860504653826984\n",
      "\tvalidation loss: 0.40231722593307495\n",
      "EPOCH 6\n",
      "\ttraining loss: 0.4012500311950264\n",
      "\tAUC: 0.6860802744611044\n",
      "\tvalidation loss: 0.4006410539150238\n",
      "EPOCH 7\n",
      "\ttraining loss: 0.40067583341814555\n",
      "\tAUC: 0.68588413625362\n",
      "\tvalidation loss: 0.4012671411037445\n",
      "EPOCH 8\n",
      "\ttraining loss: 0.4006789499309071\n",
      "\tAUC: 0.6851425496636201\n",
      "\tvalidation loss: 0.4010327160358429\n",
      "EPOCH 9\n",
      "\ttraining loss: 0.4009122100077015\n",
      "\tAUC: 0.6851495391743045\n",
      "\tvalidation loss: 0.40118101239204407\n",
      "FOLD 1\n",
      "EPOCH 0\n",
      "\ttraining loss: 0.4260543072686612\n",
      "\tAUC: 0.6788459124545341\n",
      "\tvalidation loss: 0.4024134874343872\n",
      "EPOCH 1\n",
      "\ttraining loss: 0.4045934459152345\n",
      "\tAUC: 0.6808474350934404\n",
      "\tvalidation loss: 0.4074409306049347\n",
      "EPOCH 2\n",
      "\ttraining loss: 0.40359279310819013\n",
      "\tAUC: 0.683579553493039\n",
      "\tvalidation loss: 0.4007313549518585\n",
      "EPOCH 3\n",
      "\ttraining loss: 0.4029485258276794\n",
      "\tAUC: 0.683457697228145\n",
      "\tvalidation loss: 0.39695605635643005\n",
      "EPOCH 4\n",
      "\ttraining loss: 0.4030169166600434\n",
      "\tAUC: 0.6838911601655588\n",
      "\tvalidation loss: 0.39735275506973267\n",
      "EPOCH 5\n",
      "\ttraining loss: 0.4026122394117337\n",
      "\tAUC: 0.6831765107236925\n",
      "\tvalidation loss: 0.3971187174320221\n",
      "EPOCH 6\n",
      "\ttraining loss: 0.4017232797292444\n",
      "\tAUC: 0.6829071767214349\n",
      "\tvalidation loss: 0.39909717440605164\n",
      "EPOCH 7\n",
      "\ttraining loss: 0.4016284400591187\n",
      "\tAUC: 0.6852509444374766\n",
      "\tvalidation loss: 0.3965713679790497\n",
      "EPOCH 8\n",
      "\ttraining loss: 0.40161101901029694\n",
      "\tAUC: 0.6845390517998244\n",
      "\tvalidation loss: 0.39779770374298096\n",
      "EPOCH 9\n",
      "\ttraining loss: 0.40147491247908584\n",
      "\tAUC: 0.6851045930013797\n",
      "\tvalidation loss: 0.3964754641056061\n",
      "FOLD 2\n",
      "EPOCH 0\n",
      "\ttraining loss: 0.4212395675166911\n",
      "\tAUC: 0.6772371628186351\n",
      "\tvalidation loss: 0.4047401249408722\n",
      "EPOCH 1\n",
      "\ttraining loss: 0.4027513678405663\n",
      "\tAUC: 0.6769480903740372\n",
      "\tvalidation loss: 0.4106006622314453\n",
      "EPOCH 2\n",
      "\ttraining loss: 0.4019988609363346\n",
      "\tAUC: 0.6803302822932145\n",
      "\tvalidation loss: 0.4058426022529602\n",
      "EPOCH 3\n",
      "\ttraining loss: 0.4017058409340559\n",
      "\tAUC: 0.6797614785236408\n",
      "\tvalidation loss: 0.4034322500228882\n",
      "EPOCH 4\n",
      "\ttraining loss: 0.40096766865754974\n",
      "\tAUC: 0.6823482768153002\n",
      "\tvalidation loss: 0.40248405933380127\n",
      "EPOCH 5\n",
      "\ttraining loss: 0.4014315270490245\n",
      "\tAUC: 0.6819975982834123\n",
      "\tvalidation loss: 0.40418878197669983\n",
      "EPOCH 6\n",
      "\ttraining loss: 0.4008682510999414\n",
      "\tAUC: 0.6808471485513428\n",
      "\tvalidation loss: 0.4036872386932373\n",
      "EPOCH 7\n",
      "\ttraining loss: 0.4004753187636341\n",
      "\tAUC: 0.6814558355040357\n",
      "\tvalidation loss: 0.4027736783027649\n",
      "EPOCH 8\n",
      "\ttraining loss: 0.40034626178371097\n",
      "\tAUC: 0.6800381494870748\n",
      "\tvalidation loss: 0.40341246128082275\n",
      "EPOCH 9\n",
      "\ttraining loss: 0.4009068717269836\n",
      "\tAUC: 0.6820205349914772\n",
      "\tvalidation loss: 0.4024573266506195\n",
      "FOLD 3\n",
      "EPOCH 0\n",
      "\ttraining loss: 0.4253187744748631\n",
      "\tAUC: 0.6796197087587678\n",
      "\tvalidation loss: 0.41228923201560974\n",
      "EPOCH 1\n",
      "\ttraining loss: 0.4031594750564847\n",
      "\tAUC: 0.6807752464199771\n",
      "\tvalidation loss: 0.40281179547309875\n",
      "EPOCH 2\n",
      "\ttraining loss: 0.4021768184155708\n",
      "\tAUC: 0.6820828189744121\n",
      "\tvalidation loss: 0.40221333503723145\n",
      "EPOCH 3\n",
      "\ttraining loss: 0.4017202945781757\n",
      "\tAUC: 0.68303413400771\n",
      "\tvalidation loss: 0.40210115909576416\n",
      "EPOCH 4\n",
      "\ttraining loss: 0.40156750048248513\n",
      "\tAUC: 0.6830836027838507\n",
      "\tvalidation loss: 0.4019937813282013\n",
      "EPOCH 5\n",
      "\ttraining loss: 0.40152835267261394\n",
      "\tAUC: 0.6832520278451331\n",
      "\tvalidation loss: 0.40190014243125916\n",
      "EPOCH 6\n",
      "\ttraining loss: 0.40112694297407825\n",
      "\tAUC: 0.6840016592598588\n",
      "\tvalidation loss: 0.4037379324436188\n",
      "EPOCH 7\n",
      "\ttraining loss: 0.4005054501653875\n",
      "\tAUC: 0.6835925163927317\n",
      "\tvalidation loss: 0.4024226665496826\n",
      "EPOCH 8\n",
      "\ttraining loss: 0.40052073962480117\n",
      "\tAUC: 0.6835281981095414\n",
      "\tvalidation loss: 0.4018171429634094\n",
      "EPOCH 9\n",
      "\ttraining loss: 0.40035562713940936\n",
      "\tAUC: 0.6854954940324505\n",
      "\tvalidation loss: 0.4011988937854767\n",
      "FOLD 4\n",
      "EPOCH 0\n",
      "\ttraining loss: 0.4246446335971548\n",
      "\tAUC: 0.6736908463183684\n",
      "\tvalidation loss: 0.4036442041397095\n",
      "EPOCH 1\n",
      "\ttraining loss: 0.40310313313910107\n",
      "\tAUC: 0.675934773861179\n",
      "\tvalidation loss: 0.4036323130130768\n",
      "EPOCH 2\n",
      "\ttraining loss: 0.40254927752087416\n",
      "\tAUC: 0.6766343031887585\n",
      "\tvalidation loss: 0.40466800332069397\n",
      "EPOCH 3\n",
      "\ttraining loss: 0.4019433794283944\n",
      "\tAUC: 0.6774699399989297\n",
      "\tvalidation loss: 0.40301549434661865\n",
      "EPOCH 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/kz7ljbl93cs7d61qrkfn3js00000gn/T/ipykernel_58959/695663687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0;31m# update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "n_epochs = 10 # paper used 100 originally\n",
    "lr = 0.001\n",
    "n_folds = 5\n",
    "\n",
    "val_loss_hist_folds = []\n",
    "train_loss_hist_folds = []\n",
    "auc_hist_folds = []\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True)\n",
    "ds_train = CustomDataset(X_train_numeric, y_train_numeric)\n",
    "len_full = len(ds_train)\n",
    "len_train = int(0.8*len(ds_train))\n",
    "len_val = len_full - len_train\n",
    "for p in range(6):\n",
    "    dropout_rate = p / 10\n",
    "    print(f\"DROPOUT RATE: {dropout_rate}\")\n",
    "    for fold_num, (train_indices, val_indices) in enumerate(kfold.split(ds_train)):\n",
    "        print(f\"FOLD {fold_num}\")\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, 3), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1, 3), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(5056, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        val_dataset = torch.utils.data.Subset(ds_train, val_indices)\n",
    "        train_dataset = torch.utils.data.Subset(ds_train, train_indices)\n",
    "\n",
    "        # Define data loaders for train and val\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                        train_dataset, \n",
    "                        batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                        val_dataset,\n",
    "                        batch_size=len_val, shuffle=False)\n",
    "        train_loss_hist = []\n",
    "        val_loss_hist = []\n",
    "        auc_hist = []\n",
    "        for epoch in range(n_epochs):\n",
    "            print(f'EPOCH {epoch}')\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                features, labels = data\n",
    "                # print(type(features))\n",
    "                # print(f\"PESKY BASTARDS: {features.isnan().sum()}\")\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass\n",
    "                output = model(features)\n",
    "                output = torch.reshape(output, (output.shape[0], ))\n",
    "                if output.isnan().sum() > 0:\n",
    "                    print(f\"RIP{output.isnan().sum()}\")\n",
    "                output = torch.nan_to_num(output)\n",
    "                # calculate loss\n",
    "                loss = criterion(output, labels)\n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "                # update\n",
    "                optimizer.step()\n",
    "                # track training loss\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_loader)\n",
    "            print(f\"\\ttraining loss: {train_loss}\")\n",
    "            train_loss_hist.append(train_loss)\n",
    "\n",
    "            # validate\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                output = None\n",
    "                labels = None\n",
    "                val_loss = 0\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    features, labels = data\n",
    "                    output = model(features)\n",
    "                    output = torch.reshape(output, (output.shape[0], ))\n",
    "                    output = torch.nan_to_num(output)\n",
    "                    loss = criterion(output, labels)\n",
    "                    val_loss += loss.item()\n",
    "                preds = output.cpu().detach().numpy()\n",
    "                ground_truths = labels.cpu().detach().numpy()\n",
    "                print(f'\\tAUC: {sklearn.metrics.roc_auc_score(ground_truths, preds)}')\n",
    "                val_loss /= len(val_loader)\n",
    "                print(f\"\\tvalidation loss: {val_loss}\")\n",
    "                val_loss_hist.append(val_loss)\n",
    "        train_loss_hist_folds.append(train_loss_hist)\n",
    "        val_loss_hist_folds.append(val_loss_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "\ttraining loss: 0.42515635166143506\n",
      "EPOCH 1\n",
      "\ttraining loss: 0.4053219809278923\n",
      "EPOCH 2\n",
      "\ttraining loss: 0.40451630131568317\n",
      "EPOCH 3\n",
      "\ttraining loss: 0.4039568227035394\n"
     ]
    }
   ],
   "source": [
    "# train on entire dataset\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "n_epochs = 4 # paper used 100 originally\n",
    "lr = 0.001\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, 3), stride=1, padding=0),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=(1, 2), stride=1),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1, 3), stride=1, padding=0),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=(1, 2), stride=1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(5056, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define data loaders for train and val\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    ds_train, \n",
    "                    batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'EPOCH {epoch}')\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        features, labels = data\n",
    "        # print(type(features))\n",
    "        # print(f\"PESKY BASTARDS: {features.isnan().sum()}\")\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(features)\n",
    "        output = torch.reshape(output, (output.shape[0], ))\n",
    "        if output.isnan().sum() > 0:\n",
    "            print(f\"RIP{output.isnan().sum()}\")\n",
    "        output = torch.nan_to_num(output)\n",
    "        # calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update\n",
    "        optimizer.step()\n",
    "        # track training loss\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"\\ttraining loss: {train_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b5295d72cc8c3d140bbb6686d5919ce0ad0a523816efde1e1cd082b7d39dbc7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
